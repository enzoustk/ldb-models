{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b597100",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "import cmdstanpy\n",
    "import arviz as az\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from __future__ import annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2fe0ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "STAN_FILE = \"models/v3/ldb_pace_model.stan\"     \n",
    "OUT_DIR = Path(\"models/v3\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "cmdstanpy.set_cmdstan_path(\"/home/enzou/cmdstan\") \n",
    "\n",
    "SEED = 42\n",
    "WARMUP = 2000\n",
    "SAMPLE = 2000\n",
    "CHAINS = 4\n",
    "PARALLEL = 4\n",
    "MAX_TREEDEPTH = 12\n",
    "ADAPT_DELTA = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127bd1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_overtimes_and_incomplete(df: pd.DataFrame, base_periods=(1, 2, 3, 4)) -> pd.DataFrame:\n",
    "    \"\"\"Remove OTs e descarta jogos sem todos os 4 períodos base.\"\"\"\n",
    "    tmp = df.copy()\n",
    "\n",
    "    # Manter somente períodos numéricos\n",
    "    tmp[\"periodo_num\"] = pd.to_numeric(tmp[\"periodo\"], errors=\"coerce\")\n",
    "    tmp = tmp[tmp[\"periodo_num\"].notna()].copy()\n",
    "    tmp[\"periodo_num\"] = tmp[\"periodo_num\"].astype(int)\n",
    "\n",
    "    # Filtro 1..4\n",
    "    tmp = tmp[tmp[\"periodo_num\"].isin(base_periods)].copy()\n",
    "\n",
    "    # Garantir jogos completos (todos os 4 períodos)\n",
    "    cnt = tmp.groupby(\"hash_partida\")[\"periodo_num\"].nunique()\n",
    "    ok_games = cnt[cnt == len(base_periods)].index\n",
    "    tmp = tmp[tmp[\"hash_partida\"].isin(ok_games)].copy()\n",
    "\n",
    "    # padroniza coluna 'periodo' como int 1..4\n",
    "    tmp[\"periodo\"] = tmp[\"periodo_num\"].astype(int)\n",
    "    tmp.drop(columns=[\"periodo_num\"], inplace=True)\n",
    "\n",
    "    return tmp\n",
    "\n",
    "\n",
    "def _safe_int_series(s: pd.Series) -> np.ndarray:\n",
    "    \"\"\"Coerce->int, garantindo não-negatividade.\"\"\"\n",
    "    return np.asarray(pd.to_numeric(s, errors=\"coerce\").fillna(0).clip(lower=0), dtype=int)\n",
    "\n",
    "\n",
    "def build_long_and_game_level(train_df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Monta:\n",
    "      - df_long (linhas time x período)\n",
    "      - gp (nível jogo-período com y_poss e ids de mandante/visitante)\n",
    "      - stan_data (no formato exigido pelo Stan novo)\n",
    "      - meta (dicionários de índices)\n",
    "    \"\"\"\n",
    "    # 1) Remover OTs e jogos incompletos\n",
    "    df_base = drop_overtimes_and_incomplete(train_df, base_periods=(1, 2, 3, 4))\n",
    "\n",
    "    # 2) Formato long (A/B)\n",
    "    df_a = df_base.rename(columns={\n",
    "        'team_hash_a': 'team',\n",
    "        'team_hash_b': 'opp',\n",
    "        'fg2_att_a': 'fga2', 'fg2_made_a': 'fgm2',\n",
    "        'fg3_att_a': 'fga3', 'fg3_made_a': 'fgm3',\n",
    "        'ft_att_a':  'fta',  'ft_made_a':  'ftm',\n",
    "        'pts_a': 'pts'\n",
    "    }).assign(side=\"A\")\n",
    "\n",
    "    df_b = df_base.rename(columns={\n",
    "        'team_hash_b': 'team',\n",
    "        'team_hash_a': 'opp',\n",
    "        'fg2_att_b': 'fga2', 'fg2_made_b': 'fgm2',\n",
    "        'fg3_att_b': 'fga3', 'fg3_made_b': 'fgm3',\n",
    "        'ft_att_b':  'fta',  'ft_made_b':  'ftm',\n",
    "        'pts_b': 'pts'\n",
    "    }).assign(side=\"B\")\n",
    "\n",
    "    df_long = pd.concat([df_a, df_b], ignore_index=True)\n",
    "\n",
    "    # 3) Índices de times, períodos e jogos\n",
    "    all_teams = pd.Index(pd.unique(\n",
    "        pd.concat([df_base[\"team_hash_a\"], df_base[\"team_hash_b\"]], ignore_index=True)\n",
    "    )).sort_values()\n",
    "    team_index = {t: i + 1 for i, t in enumerate(all_teams)}\n",
    "\n",
    "    df_long[\"team_id\"] = df_long[\"team\"].map(team_index).astype(int)\n",
    "    df_long[\"opp_id\"]  = df_long[\"opp\"].map(team_index).astype(int)\n",
    "\n",
    "    period_map = {p: i + 1 for i, p in enumerate(sorted(df_base[\"periodo\"].unique()))}  # 1..4\n",
    "    df_long[\"period\"]  = df_long[\"periodo\"].map(period_map).astype(int)\n",
    "\n",
    "    # --- mapeamento ÚNICO de game_id, aplicado a df_long e gp ---\n",
    "    game_order = pd.Index(df_base[\"hash_partida\"].unique()).sort_values()\n",
    "    game_index_map = {h: i + 1 for i, h in enumerate(game_order)}\n",
    "    df_long[\"game_id\"] = df_long[\"hash_partida\"].map(game_index_map).astype(int)\n",
    "\n",
    "    # 4) Nível do jogo-período (pace): y_poss, home/away, exposure (=1.0 no treino)\n",
    "    gp = (df_base[[\"hash_partida\", \"periodo\", \"team_hash_a\", \"team_hash_b\", \"match_pace\"]]\n",
    "          .drop_duplicates()\n",
    "          .copy())\n",
    "    gp[\"game_id\"] = gp[\"hash_partida\"].map(game_index_map).astype(int)\n",
    "    gp[\"period\"]  = gp[\"periodo\"].map(period_map).astype(int)\n",
    "    gp[\"home_id\"] = gp[\"team_hash_a\"].map(team_index).astype(int)\n",
    "    gp[\"away_id\"] = gp[\"team_hash_b\"].map(team_index).astype(int)\n",
    "\n",
    "    if gp[\"match_pace\"].isna().any():\n",
    "        gp = gp.dropna(subset=[\"match_pace\"]).copy()\n",
    "\n",
    "    # y_poss: contagem inteira (NegBin2)\n",
    "    gp[\"y_poss\"] = np.rint(gp[\"match_pace\"].clip(lower=0)).astype(int)\n",
    "\n",
    "    # Matriz y_poss[G,Q] (ordenada por game_id e period)\n",
    "    gp_matrix = (gp[[\"game_id\", \"period\", \"y_poss\"]]\n",
    "                 .pivot(index=\"game_id\", columns=\"period\", values=\"y_poss\")\n",
    "                 .sort_index())\n",
    "    if gp_matrix.isna().any().any():\n",
    "        missing_games = gp_matrix.index[gp_matrix.isna().any(axis=1)].tolist()\n",
    "        if missing_games:\n",
    "            gp = gp[~gp[\"game_id\"].isin(missing_games)].copy()\n",
    "            gp_matrix = (gp[[\"game_id\", \"period\", \"y_poss\"]]\n",
    "                         .pivot(index=\"game_id\", columns=\"period\", values=\"y_poss\")\n",
    "                         .sort_index())\n",
    "\n",
    "    y_poss = gp_matrix.to_numpy(dtype=int)\n",
    "    G, Q = y_poss.shape\n",
    "\n",
    "    # Vetores home/away por jogo\n",
    "    g_home = (gp.drop_duplicates(\"game_id\")\n",
    "                .sort_values(\"game_id\")[[\"game_id\", \"home_id\", \"away_id\"]])\n",
    "    home_team = g_home[\"home_id\"].to_numpy()\n",
    "    away_team = g_home[\"away_id\"].to_numpy()\n",
    "\n",
    "    # exposure: 10min => 1.0 em treino\n",
    "    exposure_pace = np.ones((G, Q), dtype=float)\n",
    "\n",
    "    # 5) Alvos por linha (long)\n",
    "    y2a  = _safe_int_series(df_long[\"fga2\"])\n",
    "    y3a  = _safe_int_series(df_long[\"fga3\"])\n",
    "    yfta = _safe_int_series(df_long[\"fta\"])\n",
    "    y2m  = _safe_int_series(df_long[\"fgm2\"])\n",
    "    y3m  = _safe_int_series(df_long[\"fgm3\"])\n",
    "    yftm = _safe_int_series(df_long[\"ftm\"])\n",
    "\n",
    "    # Consistência: feitos <= tentados\n",
    "    for made, att in [(y2m, y2a), (y3m, y3a), (yftm, yfta)]:\n",
    "        bad = made > att\n",
    "        if bad.any():\n",
    "            made[bad] = att[bad]\n",
    "\n",
    "    stan_data = {\n",
    "        \"N\": int(len(df_long)),\n",
    "        \"T\": int(len(team_index)),\n",
    "        \"Q\": int(Q),\n",
    "        \"G\": int(G),\n",
    "        \"team\": df_long[\"team_id\"].astype(int).to_list(),\n",
    "        \"opp\": df_long[\"opp_id\"].astype(int).to_list(),\n",
    "        \"period\": df_long[\"period\"].astype(int).to_list(),\n",
    "        \"game_id\": df_long[\"game_id\"].astype(int).to_list(),\n",
    "        \"home_team\": home_team.tolist(),\n",
    "        \"away_team\": away_team.tolist(),\n",
    "        \"y_poss\": y_poss.tolist(),\n",
    "        \"exposure_pace\": exposure_pace.tolist(),\n",
    "        \"y2a\": y2a.tolist(),\n",
    "        \"y3a\": y3a.tolist(),\n",
    "        \"yfta\": yfta.tolist(),\n",
    "        \"y2m\": y2m.tolist(),\n",
    "        \"y3m\": y3m.tolist(),\n",
    "        \"yftm\": yftm.tolist(),\n",
    "    }\n",
    "\n",
    "    meta = {\n",
    "        \"team_index\": team_index,\n",
    "        \"period_map\": period_map,\n",
    "        \"home_team\": {int(r.game_id): int(r.home_id) for r in g_home.itertuples()},\n",
    "        \"away_team\": {int(r.game_id): int(r.away_id) for r in g_home.itertuples()},\n",
    "    }\n",
    "\n",
    "    return stan_data, meta, df_long, gp\n",
    "\n",
    "\n",
    "def save_metadata(\n",
    "        out_dir: Path,\n",
    "        stan_file: str,\n",
    "        stan_data: dict,\n",
    "        meta: dict,\n",
    "        df_long: pd.DataFrame,\n",
    "        gp: pd.DataFrame\n",
    "    ):\n",
    "    \"\"\"Salva index/mapeamentos essenciais para replicar o modelo.\"\"\"\n",
    "    meta_dir = out_dir / \"metadata\"\n",
    "    meta_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # team_index e inverso\n",
    "    with open(meta_dir / \"team_index.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(meta[\"team_index\"], f, ensure_ascii=False, indent=2)\n",
    "    team_index_rev = {int(v): k for k, v in meta[\"team_index\"].items()}\n",
    "    with open(meta_dir / \"team_index_rev.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(team_index_rev, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    # period_map\n",
    "    with open(meta_dir / \"period_map.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump({str(k): int(v) for k, v in meta[\"period_map\"].items()}, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    # game_index (hash -> id)\n",
    "    game_index = (df_long[[\"hash_partida\", \"game_id\"]]\n",
    "                  .drop_duplicates()\n",
    "                  .sort_values(\"game_id\"))\n",
    "    game_map = {str(r.hash_partida): int(r.game_id) for r in game_index.itertuples()}\n",
    "    with open(meta_dir / \"game_index.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(game_map, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    # home/away por jogo\n",
    "    g_home = (gp.drop_duplicates(\"game_id\")\n",
    "                .sort_values(\"game_id\")[[\"game_id\", \"home_id\", \"away_id\"]])\n",
    "    home_away = {int(r.game_id): {\"home_team_id\": int(r.home_id), \"away_team_id\": int(r.away_id)}\n",
    "                 for r in g_home.itertuples()}\n",
    "    with open(meta_dir / \"home_away.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(home_away, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    # manifest\n",
    "    manifest = {\n",
    "        \"stan_file\": stan_file,\n",
    "        \"dims\": {\"N\": stan_data[\"N\"], \"T\": stan_data[\"T\"], \"Q\": stan_data[\"Q\"], \"G\": stan_data[\"G\"]},\n",
    "        \"base_minutes\": 10,\n",
    "        \"variables\": {\n",
    "            \"pace\": [\"y_poss\", \"exposure_pace\", \"home_team\", \"away_team\"],\n",
    "            \"shots\": [\"y2a\", \"y3a\", \"yfta\", \"y2m\", \"y3m\", \"yftm\"],\n",
    "            \"indexing\": [\"team\", \"opp\", \"period\", \"game_id\"]\n",
    "        }\n",
    "    }\n",
    "    with open(meta_dir / \"manifest.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(manifest, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    # opcional: exportar nível do jogo\n",
    "    gp_out = gp[[\"hash_partida\", \"game_id\", \"period\", \"home_id\", \"away_id\", \"y_poss\"]].copy()\n",
    "    gp_out.to_csv(meta_dir / \"game_level.csv\", index=False, encoding=\"utf-8\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4585269",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "        train_df: pd.DataFrame,\n",
    "        stan_file: str = STAN_FILE,\n",
    "        out_dir: Path = OUT_DIR\n",
    "    ):\n",
    "    \"\"\"Treina o Stan e salva artefatos.\"\"\"\n",
    "    stan_data, meta, df_long, gp = build_long_and_game_level(train_df)\n",
    "\n",
    "    print(f\"[INFO] T={stan_data['T']} times, G={stan_data['G']} jogos, Q={stan_data['Q']} períodos, N={stan_data['N']} linhas.\")\n",
    "    print(f\"[INFO] Compilando Stan em: {stan_file}\")\n",
    "\n",
    "    model = cmdstanpy.CmdStanModel(stan_file=stan_file)\n",
    "\n",
    "    fit = model.sample(\n",
    "        data=stan_data,\n",
    "        seed=SEED,\n",
    "        iter_warmup=WARMUP,\n",
    "        iter_sampling=SAMPLE,\n",
    "        chains=CHAINS,\n",
    "        parallel_chains=PARALLEL,\n",
    "        max_treedepth=MAX_TREEDEPTH,\n",
    "        adapt_delta=ADAPT_DELTA,\n",
    "        show_progress=True,\n",
    "    )\n",
    "\n",
    "    print(\"[INFO] Convertendo para ArviZ...\")\n",
    "    # Informe explicitamente as variáveis de log-likelihood\n",
    "    idata = az.from_cmdstanpy(\n",
    "        posterior=fit,\n",
    "        log_likelihood=[\"log_lik_shots\", \"log_lik_pace\"],\n",
    "        coords={\n",
    "            \"obs_id\": np.arange(stan_data[\"N\"]),\n",
    "            \"gp_id\":  np.arange(stan_data[\"G\"] * stan_data[\"Q\"]),\n",
    "        },\n",
    "        dims={\n",
    "            \"log_lik_shots\": [\"obs_id\"],\n",
    "            \"log_lik_pace\":  [\"gp_id\"],\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # Escolhe qual loglik usar no LOO (preferimos o de arremessos)\n",
    "    ll_vars = list(getattr(idata, \"log_likelihood\").data_vars)\n",
    "    target_ll = \"log_lik_shots\" if \"log_lik_shots\" in ll_vars else ll_vars[0]\n",
    "    loo = az.loo(idata, var_name=target_ll, pointwise=True)\n",
    "    print(loo)\n",
    "\n",
    "    # Salva artefatos principais\n",
    "    (out_dir / \"draws\").mkdir(parents=True, exist_ok=True)\n",
    "    fit.save_csvfiles(str(out_dir / \"draws\"))\n",
    "\n",
    "    summary_df = fit.summary()\n",
    "    summary_df.to_parquet(out_dir / \"summary.parquet\")\n",
    "\n",
    "    with open(out_dir / \"idata.pkl\", \"wb\") as f:\n",
    "        pickle.dump(idata, f)\n",
    "    with open(out_dir / \"stan_data.pkl\", \"wb\") as f:\n",
    "        pickle.dump(stan_data, f)\n",
    "    with open(out_dir / \"loo.txt\", \"w\") as f:\n",
    "        f.write(str(loo))\n",
    "    with open(out_dir / \"loo.pkl\", \"wb\") as f:\n",
    "        pickle.dump(loo, f)\n",
    "\n",
    "    save_metadata(out_dir, stan_file, stan_data, meta, df_long, gp)\n",
    "\n",
    "    print(f\"✅ Treino concluído e artefatos salvos em {out_dir}\")\n",
    "    return fit, idata, stan_data, meta"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv-WSL)",
   "language": "python",
   "name": "venv-wsl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
