{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b597100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Treino do modelo Stan (pace AR(1) com match exposure), sem prorrogações.\n",
    "- Converte wide -> long\n",
    "- Remove OTs e jogos incompletos (1..4 períodos)\n",
    "- Monta stan_data (y_poss[G,Q], exposure_pace[G,Q], home/away, y2a/y3a/...)\n",
    "- Treina com CmdStanPy\n",
    "- Salva draws, summary, idata, stan_data e metadados (índices, mapeamentos)\n",
    "\n",
    "Requisitos:\n",
    "  pip install pandas numpy arviz cmdstanpy pyarrow\n",
    "  cmdstanpy.install_cmdstan()  # uma única vez no ambiente\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "from pathlib import Path\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import arviz as az\n",
    "import cmdstanpy\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "STAN_FILE = \"models/v2/ldb_pace_model.stan\"     # .stan atualizado com exposure_pace\n",
    "OUT_DIR = Path(\"models/v2\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "cmdstanpy.set_cmdstan_path(\"/home/enzou/cmdstan\")  # ajuste se necessário\n",
    "\n",
    "SEED = 42\n",
    "WARMUP = 2000\n",
    "SAMPLE = 2000\n",
    "CHAINS = 4\n",
    "PARALLEL = 4\n",
    "MAX_TREEDEPTH = 12\n",
    "ADAPT_DELTA = 0.95\n",
    "\n",
    "# =========================\n",
    "# HELPERS\n",
    "# =========================\n",
    "def drop_overtimes_and_incomplete(df: pd.DataFrame, base_periods=(1, 2, 3, 4)) -> pd.DataFrame:\n",
    "    \"\"\"Remove OTs e descarta jogos sem todos os 4 períodos base.\"\"\"\n",
    "    tmp = df.copy()\n",
    "\n",
    "    # Manter somente períodos numéricos\n",
    "    tmp[\"periodo_num\"] = pd.to_numeric(tmp[\"periodo\"], errors=\"coerce\")\n",
    "    tmp = tmp[tmp[\"periodo_num\"].notna()].copy()\n",
    "    tmp[\"periodo_num\"] = tmp[\"periodo_num\"].astype(int)\n",
    "\n",
    "    # Filtro 1..4\n",
    "    tmp = tmp[tmp[\"periodo_num\"].isin(base_periods)].copy()\n",
    "\n",
    "    # Garantir jogos completos (todos os 4 períodos)\n",
    "    cnt = tmp.groupby(\"hash_partida\")[\"periodo_num\"].nunique()\n",
    "    ok_games = cnt[cnt == len(base_periods)].index\n",
    "    tmp = tmp[tmp[\"hash_partida\"].isin(ok_games)].copy()\n",
    "\n",
    "    # padroniza coluna 'periodo' como int 1..4\n",
    "    tmp[\"periodo\"] = tmp[\"periodo_num\"].astype(int)\n",
    "    tmp.drop(columns=[\"periodo_num\"], inplace=True)\n",
    "\n",
    "    return tmp\n",
    "\n",
    "\n",
    "def _safe_int_series(s: pd.Series) -> np.ndarray:\n",
    "    \"\"\"Coerce->int, garantindo não-negatividade.\"\"\"\n",
    "    return np.asarray(pd.to_numeric(s, errors=\"coerce\").fillna(0).clip(lower=0), dtype=int)\n",
    "\n",
    "\n",
    "def build_long_and_game_level(train_df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Monta:\n",
    "      - df_long (linhas time x período)\n",
    "      - gp (nível jogo-período com y_poss e ids de mandante/visitante)\n",
    "      - stan_data (no formato exigido pelo Stan novo)\n",
    "      - meta (dicionários de índices)\n",
    "    \"\"\"\n",
    "    # 1) Remover OTs e jogos incompletos\n",
    "    df_base = drop_overtimes_and_incomplete(train_df, base_periods=(1, 2, 3, 4))\n",
    "\n",
    "    # 2) Formato long (A/B)\n",
    "    df_a = df_base.rename(columns={\n",
    "        'team_hash_a': 'team',\n",
    "        'team_hash_b': 'opp',\n",
    "        'fg2_att_a': 'fga2', 'fg2_made_a': 'fgm2',\n",
    "        'fg3_att_a': 'fga3', 'fg3_made_a': 'fgm3',\n",
    "        'ft_att_a':  'fta',  'ft_made_a':  'ftm',\n",
    "        'pts_a': 'pts'\n",
    "    }).assign(side=\"A\")\n",
    "\n",
    "    df_b = df_base.rename(columns={\n",
    "        'team_hash_b': 'team',\n",
    "        'team_hash_a': 'opp',\n",
    "        'fg2_att_b': 'fga2', 'fg2_made_b': 'fgm2',\n",
    "        'fg3_att_b': 'fga3', 'fg3_made_b': 'fgm3',\n",
    "        'ft_att_b':  'fta',  'ft_made_b':  'ftm',\n",
    "        'pts_b': 'pts'\n",
    "    }).assign(side=\"B\")\n",
    "\n",
    "    df_long = pd.concat([df_a, df_b], ignore_index=True)\n",
    "\n",
    "    # 3) Índices de times, períodos e jogos\n",
    "    all_teams = pd.Index(pd.unique(\n",
    "        pd.concat([df_base[\"team_hash_a\"], df_base[\"team_hash_b\"]], ignore_index=True)\n",
    "    )).sort_values()\n",
    "    team_index = {t: i + 1 for i, t in enumerate(all_teams)}\n",
    "\n",
    "    df_long[\"team_id\"] = df_long[\"team\"].map(team_index).astype(int)\n",
    "    df_long[\"opp_id\"]  = df_long[\"opp\"].map(team_index).astype(int)\n",
    "\n",
    "    period_map = {p: i + 1 for i, p in enumerate(sorted(df_base[\"periodo\"].unique()))}  # 1..4\n",
    "    df_long[\"period\"]  = df_long[\"periodo\"].map(period_map).astype(int)\n",
    "\n",
    "    # --- mapeamento ÚNICO de game_id, aplicado a df_long e gp ---\n",
    "    game_order = pd.Index(df_base[\"hash_partida\"].unique()).sort_values()\n",
    "    game_index_map = {h: i + 1 for i, h in enumerate(game_order)}\n",
    "    df_long[\"game_id\"] = df_long[\"hash_partida\"].map(game_index_map).astype(int)\n",
    "\n",
    "    # 4) Nível do jogo-período (pace): y_poss, home/away, exposure (=1.0 no treino)\n",
    "    gp = (df_base[[\"hash_partida\", \"periodo\", \"team_hash_a\", \"team_hash_b\", \"match_pace\"]]\n",
    "          .drop_duplicates()\n",
    "          .copy())\n",
    "    gp[\"game_id\"] = gp[\"hash_partida\"].map(game_index_map).astype(int)\n",
    "    gp[\"period\"]  = gp[\"periodo\"].map(period_map).astype(int)\n",
    "    gp[\"home_id\"] = gp[\"team_hash_a\"].map(team_index).astype(int)\n",
    "    gp[\"away_id\"] = gp[\"team_hash_b\"].map(team_index).astype(int)\n",
    "\n",
    "    if gp[\"match_pace\"].isna().any():\n",
    "        gp = gp.dropna(subset=[\"match_pace\"]).copy()\n",
    "\n",
    "    # y_poss: contagem inteira (NegBin2)\n",
    "    gp[\"y_poss\"] = np.rint(gp[\"match_pace\"].clip(lower=0)).astype(int)\n",
    "\n",
    "    # Matriz y_poss[G,Q] (ordenada por game_id e period)\n",
    "    gp_matrix = (gp[[\"game_id\", \"period\", \"y_poss\"]]\n",
    "                 .pivot(index=\"game_id\", columns=\"period\", values=\"y_poss\")\n",
    "                 .sort_index())\n",
    "    if gp_matrix.isna().any().any():\n",
    "        missing_games = gp_matrix.index[gp_matrix.isna().any(axis=1)].tolist()\n",
    "        if missing_games:\n",
    "            gp = gp[~gp[\"game_id\"].isin(missing_games)].copy()\n",
    "            gp_matrix = (gp[[\"game_id\", \"period\", \"y_poss\"]]\n",
    "                         .pivot(index=\"game_id\", columns=\"period\", values=\"y_poss\")\n",
    "                         .sort_index())\n",
    "\n",
    "    y_poss = gp_matrix.to_numpy(dtype=int)\n",
    "    G, Q = y_poss.shape\n",
    "\n",
    "    # Vetores home/away por jogo\n",
    "    g_home = (gp.drop_duplicates(\"game_id\")\n",
    "                .sort_values(\"game_id\")[[\"game_id\", \"home_id\", \"away_id\"]])\n",
    "    home_team = g_home[\"home_id\"].to_numpy()\n",
    "    away_team = g_home[\"away_id\"].to_numpy()\n",
    "\n",
    "    # exposure: 10min => 1.0 em treino\n",
    "    exposure_pace = np.ones((G, Q), dtype=float)\n",
    "\n",
    "    # 5) Alvos por linha (long)\n",
    "    y2a  = _safe_int_series(df_long[\"fga2\"])\n",
    "    y3a  = _safe_int_series(df_long[\"fga3\"])\n",
    "    yfta = _safe_int_series(df_long[\"fta\"])\n",
    "    y2m  = _safe_int_series(df_long[\"fgm2\"])\n",
    "    y3m  = _safe_int_series(df_long[\"fgm3\"])\n",
    "    yftm = _safe_int_series(df_long[\"ftm\"])\n",
    "\n",
    "    # Consistência: feitos <= tentados\n",
    "    for made, att in [(y2m, y2a), (y3m, y3a), (yftm, yfta)]:\n",
    "        bad = made > att\n",
    "        if bad.any():\n",
    "            made[bad] = att[bad]\n",
    "\n",
    "    stan_data = {\n",
    "        \"N\": int(len(df_long)),\n",
    "        \"T\": int(len(team_index)),\n",
    "        \"Q\": int(Q),\n",
    "        \"G\": int(G),\n",
    "        \"team\": df_long[\"team_id\"].astype(int).to_list(),\n",
    "        \"opp\": df_long[\"opp_id\"].astype(int).to_list(),\n",
    "        \"period\": df_long[\"period\"].astype(int).to_list(),\n",
    "        \"game_id\": df_long[\"game_id\"].astype(int).to_list(),\n",
    "        \"home_team\": home_team.tolist(),\n",
    "        \"away_team\": away_team.tolist(),\n",
    "        \"y_poss\": y_poss.tolist(),\n",
    "        \"exposure_pace\": exposure_pace.tolist(),\n",
    "        \"y2a\": y2a.tolist(),\n",
    "        \"y3a\": y3a.tolist(),\n",
    "        \"yfta\": yfta.tolist(),\n",
    "        \"y2m\": y2m.tolist(),\n",
    "        \"y3m\": y3m.tolist(),\n",
    "        \"yftm\": yftm.tolist(),\n",
    "    }\n",
    "\n",
    "    meta = {\n",
    "        \"team_index\": team_index,\n",
    "        \"period_map\": period_map,\n",
    "        \"home_team\": {int(r.game_id): int(r.home_id) for r in g_home.itertuples()},\n",
    "        \"away_team\": {int(r.game_id): int(r.away_id) for r in g_home.itertuples()},\n",
    "    }\n",
    "\n",
    "    return stan_data, meta, df_long, gp\n",
    "\n",
    "\n",
    "def save_metadata(out_dir: Path, stan_file: str, stan_data: dict, meta: dict,\n",
    "                  df_long: pd.DataFrame, gp: pd.DataFrame):\n",
    "    \"\"\"Salva index/mapeamentos essenciais para replicar o modelo.\"\"\"\n",
    "    meta_dir = out_dir / \"metadata\"\n",
    "    meta_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # team_index e inverso\n",
    "    with open(meta_dir / \"team_index.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(meta[\"team_index\"], f, ensure_ascii=False, indent=2)\n",
    "    team_index_rev = {int(v): k for k, v in meta[\"team_index\"].items()}\n",
    "    with open(meta_dir / \"team_index_rev.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(team_index_rev, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    # period_map\n",
    "    with open(meta_dir / \"period_map.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump({str(k): int(v) for k, v in meta[\"period_map\"].items()}, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    # game_index (hash -> id)\n",
    "    game_index = (df_long[[\"hash_partida\", \"game_id\"]]\n",
    "                  .drop_duplicates()\n",
    "                  .sort_values(\"game_id\"))\n",
    "    game_map = {str(r.hash_partida): int(r.game_id) for r in game_index.itertuples()}\n",
    "    with open(meta_dir / \"game_index.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(game_map, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    # home/away por jogo\n",
    "    g_home = (gp.drop_duplicates(\"game_id\")\n",
    "                .sort_values(\"game_id\")[[\"game_id\", \"home_id\", \"away_id\"]])\n",
    "    home_away = {int(r.game_id): {\"home_team_id\": int(r.home_id), \"away_team_id\": int(r.away_id)}\n",
    "                 for r in g_home.itertuples()}\n",
    "    with open(meta_dir / \"home_away.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(home_away, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    # manifest\n",
    "    manifest = {\n",
    "        \"stan_file\": stan_file,\n",
    "        \"dims\": {\"N\": stan_data[\"N\"], \"T\": stan_data[\"T\"], \"Q\": stan_data[\"Q\"], \"G\": stan_data[\"G\"]},\n",
    "        \"base_minutes\": 10,\n",
    "        \"variables\": {\n",
    "            \"pace\": [\"y_poss\", \"exposure_pace\", \"home_team\", \"away_team\"],\n",
    "            \"shots\": [\"y2a\", \"y3a\", \"yfta\", \"y2m\", \"y3m\", \"yftm\"],\n",
    "            \"indexing\": [\"team\", \"opp\", \"period\", \"game_id\"]\n",
    "        }\n",
    "    }\n",
    "    with open(meta_dir / \"manifest.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(manifest, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    # opcional: exportar nível do jogo\n",
    "    gp_out = gp[[\"hash_partida\", \"game_id\", \"period\", \"home_id\", \"away_id\", \"y_poss\"]].copy()\n",
    "    gp_out.to_csv(meta_dir / \"game_level.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "\n",
    "def train_model(train_df: pd.DataFrame,\n",
    "                stan_file: str = STAN_FILE,\n",
    "                out_dir: Path = OUT_DIR):\n",
    "    \"\"\"Treina o Stan e salva artefatos.\"\"\"\n",
    "    stan_data, meta, df_long, gp = build_long_and_game_level(train_df)\n",
    "\n",
    "    print(f\"[INFO] T={stan_data['T']} times, G={stan_data['G']} jogos, Q={stan_data['Q']} períodos, N={stan_data['N']} linhas.\")\n",
    "    print(f\"[INFO] Compilando Stan em: {stan_file}\")\n",
    "\n",
    "    model = cmdstanpy.CmdStanModel(stan_file=stan_file)\n",
    "\n",
    "    fit = model.sample(\n",
    "        data=stan_data,\n",
    "        seed=SEED,\n",
    "        iter_warmup=WARMUP,\n",
    "        iter_sampling=SAMPLE,\n",
    "        chains=CHAINS,\n",
    "        parallel_chains=PARALLEL,\n",
    "        max_treedepth=MAX_TREEDEPTH,\n",
    "        adapt_delta=ADAPT_DELTA,\n",
    "        show_progress=True,\n",
    "    )\n",
    "\n",
    "    print(\"[INFO] Convertendo para ArviZ...\")\n",
    "    # Informe explicitamente as variáveis de log-likelihood\n",
    "    idata = az.from_cmdstanpy(\n",
    "        posterior=fit,\n",
    "        log_likelihood=[\"log_lik_shots\", \"log_lik_pace\"],\n",
    "        coords={\n",
    "            \"obs_id\": np.arange(stan_data[\"N\"]),\n",
    "            \"gp_id\":  np.arange(stan_data[\"G\"] * stan_data[\"Q\"]),\n",
    "        },\n",
    "        dims={\n",
    "            \"log_lik_shots\": [\"obs_id\"],\n",
    "            \"log_lik_pace\":  [\"gp_id\"],\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # Escolhe qual loglik usar no LOO (preferimos o de arremessos)\n",
    "    ll_vars = list(getattr(idata, \"log_likelihood\").data_vars)\n",
    "    target_ll = \"log_lik_shots\" if \"log_lik_shots\" in ll_vars else ll_vars[0]\n",
    "    loo = az.loo(idata, var_name=target_ll, pointwise=True)\n",
    "    print(loo)\n",
    "\n",
    "    # Salva artefatos principais\n",
    "    (out_dir / \"draws\").mkdir(parents=True, exist_ok=True)\n",
    "    fit.save_csvfiles(str(out_dir / \"draws\"))\n",
    "\n",
    "    summary_df = fit.summary()\n",
    "    summary_df.to_parquet(out_dir / \"summary.parquet\")\n",
    "\n",
    "    with open(out_dir / \"idata.pkl\", \"wb\") as f:\n",
    "        pickle.dump(idata, f)\n",
    "    with open(out_dir / \"stan_data.pkl\", \"wb\") as f:\n",
    "        pickle.dump(stan_data, f)\n",
    "    with open(out_dir / \"loo.txt\", \"w\") as f:\n",
    "        f.write(str(loo))\n",
    "    with open(out_dir / \"loo.pkl\", \"wb\") as f:\n",
    "        pickle.dump(loo, f)\n",
    "\n",
    "    save_metadata(out_dir, stan_file, stan_data, meta, df_long, gp)\n",
    "\n",
    "    print(f\"✅ Treino concluído e artefatos salvos em {out_dir}\")\n",
    "    return fit, idata, stan_data, meta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7807e25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09:51:01 - cmdstanpy - INFO - CmdStan start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] T=72 times, G=387 jogos, Q=4 períodos, N=3096 linhas.\n",
      "[INFO] Compilando Stan em: models/v2/ldb_pace_model.stan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "chain 1 |\u001b[34m██████████\u001b[0m| 52:20 Sampling completed                       \n",
      "chain 2 |\u001b[34m██████████\u001b[0m| 52:20 Sampling completed                       \n",
      "chain 3 |\u001b[34m██████████\u001b[0m| 52:20 Sampling completed                       \n",
      "chain 4 |\u001b[34m██████████\u001b[0m| 52:20 Sampling completed                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                                                                                                                                                "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "10:43:22 - cmdstanpy - INFO - CmdStan done processing.\n",
      "10:43:22 - cmdstanpy - WARNING - Non-fatal error during sampling:\n",
      "Exception: neg_binomial_2_log_lpmf: Log location parameter is -inf, but must be finite! (in 'ldb_pace_model.stan', line 235, column 4 to column 51)\n",
      "Exception: neg_binomial_2_log_lpmf: Log location parameter is -inf, but must be finite! (in 'ldb_pace_model.stan', line 160, column 6 to column 64)\n",
      "Consider re-running with show_console=True if the above output is unclear!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10:43:26 - cmdstanpy - WARNING - Some chains may have failed to converge.\n",
      "\tChain 3 had 2 divergent transitions (0.1%)\n",
      "\tUse the \"diagnose()\" method on the CmdStanMCMC object to see further information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Convertendo para ArviZ...\n",
      "Computed from 8000 posterior samples and 3096 observations log-likelihood matrix.\n",
      "\n",
      "         Estimate       SE\n",
      "elpd_loo -36683.54    89.71\n",
      "p_loo      443.79        -\n",
      "------\n",
      "\n",
      "Pareto k diagnostic values:\n",
      "                         Count   Pct.\n",
      "(-Inf, 0.70]   (good)     3096  100.0%\n",
      "   (0.70, 1]   (bad)         0    0.0%\n",
      "   (1, Inf)   (very bad)    0    0.0%\n",
      "\n",
      "✅ Treino concluído e artefatos salvos em models/v2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# =========================\n",
    "# Exemplo de uso (descomente para rodar direto)\n",
    "# =========================\n",
    "if __name__ == \"__main__\":\n",
    "    train_df = pd.read_csv(\"data/quarters_df/quarters_train_df.csv\", parse_dates=['data_partida'])\n",
    "    df = train_df[train_df[\"periodo\"].isin(['1', '2', '3', '4'])]\n",
    "    # (opcional) filtrar apenas '1','2','3','4' aqui; o builder já remove OTs e jogos incompletos\n",
    "    # train_df = train_df[train_df[\"periodo\"].isin(['1','2','3','4'])].copy()\n",
    "    fit, idata, stan_data, meta = train_model(train_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1bf6796e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking sampler transitions treedepth.\n",
      "Treedepth satisfactory for all transitions.\n",
      "\n",
      "Checking sampler transitions for divergences.\n",
      "2 of 8000 (0.03%) transitions ended with a divergence.\n",
      "These divergent transitions indicate that HMC is not fully able to explore the posterior distribution.\n",
      "Try increasing adapt delta closer to 1.\n",
      "If this doesn't remove all divergences, try to reparameterize the model.\n",
      "\n",
      "Checking E-BFMI - sampler transitions HMC potential energy.\n",
      "E-BFMI satisfactory.\n",
      "\n",
      "Rank-normalized split effective sample size satisfactory for all parameters.\n",
      "\n",
      "Rank-normalized split R-hat values satisfactory for all parameters.\n",
      "\n",
      "Processing complete.\n",
      "\n",
      "Empty DataFrame\n",
      "Columns: [Mean, MCSE, StdDev, MAD, 5%, 50%, 95%, ESS_bulk, ESS_tail, ESS_bulk/s, R_hat]\n",
      "Index: []\n",
      "                           Mean      MCSE     StdDev        MAD            5%  \\\n",
      "sd_pace_away           0.013434  0.000203   0.007368   0.008027      0.001615   \n",
      "sd_pace_home           0.019991  0.000201   0.007740   0.007529      0.005997   \n",
      "sd_def_3m              0.069718  0.000755   0.030168   0.029696      0.015157   \n",
      "sd_def_ftm             0.058781  0.000737   0.031848   0.034354      0.006863   \n",
      "sd_q_2m                0.049306  0.001005   0.054034   0.028167      0.004509   \n",
      "sd_q_2a                0.022283  0.000625   0.032697   0.012810      0.001323   \n",
      "lp__              -70037.000000  0.893116  41.999700  41.526900 -70107.400000   \n",
      "pace_away[45]         -0.015844  0.000333   0.016457   0.016278     -0.047393   \n",
      "sd_q_3a                0.069491  0.001298   0.063133   0.029494      0.021289   \n",
      "log_lik_pace[350]     -2.753400  0.001938   0.097454   0.100810     -2.911710   \n",
      "eta_pace[88,2]         2.948490  0.000542   0.027485   0.027609      2.900290   \n",
      "mu_poss[88,2]         19.084300  0.010315   0.521833   0.529306     18.179500   \n",
      "log_lik_pace[351]     -2.602500  0.001454   0.073484   0.075990     -2.724360   \n",
      "log_lik_pace[352]     -2.602390  0.001450   0.073385   0.076002     -2.723880   \n",
      "mu_poss[88,3]         19.087200  0.010248   0.523393   0.525896     18.184900   \n",
      "eta_pace[88,3]         2.948640  0.000539   0.027569   0.027496      2.900590   \n",
      "eta_pace[88,4]         2.948610  0.000537   0.027517   0.027495      2.899920   \n",
      "mu_poss[88,4]         19.086600  0.010223   0.522458   0.525069     18.172600   \n",
      "log_lik_pace[349]     -2.753620  0.001972   0.100082   0.101927     -2.919610   \n",
      "eta_pace[88,1]         2.948460  0.000551   0.028195   0.027916      2.897970   \n",
      "\n",
      "                            50%           95%  ESS_bulk  ESS_tail  ESS_bulk/s  \\\n",
      "sd_pace_away           0.013356      0.025789   1293.37   2120.11    0.314061   \n",
      "sd_pace_home           0.020397      0.032125   1524.95   1681.79    0.370294   \n",
      "sd_def_3m              0.071430      0.116728   1601.74   1684.81    0.388942   \n",
      "sd_def_ftm             0.058831      0.111721   1833.64   2514.27    0.445251   \n",
      "sd_q_2m                0.034905      0.143409   2138.09   3387.75    0.519178   \n",
      "sd_q_2a                0.014049      0.066112   2141.92   3692.65    0.520109   \n",
      "lp__              -70036.400000 -69968.600000   2224.39   4067.63    0.540134   \n",
      "pace_away[45]         -0.012618      0.003980   2454.24   5659.11    0.595947   \n",
      "sd_q_3a                0.051205      0.176080   2466.00   3597.57    0.598802   \n",
      "log_lik_pace[350]     -2.755000     -2.591190   2567.04   5175.90    0.623340   \n",
      "eta_pace[88,2]         2.950630      2.989980   2568.40   5229.49    0.623670   \n",
      "mu_poss[88,2]         19.118100     19.885200   2568.41   5229.49    0.623671   \n",
      "log_lik_pace[351]     -2.601610     -2.483310   2602.22   5234.77    0.631882   \n",
      "log_lik_pace[352]     -2.601440     -2.482680   2610.52   5212.35    0.633896   \n",
      "mu_poss[88,3]         19.117200     19.889900   2614.75   5247.42    0.634924   \n",
      "eta_pace[88,3]         2.950590      2.990210   2614.76   5247.42    0.634926   \n",
      "eta_pace[88,4]         2.950590      2.990010   2616.67   5252.88    0.635390   \n",
      "mu_poss[88,4]         19.117200     19.885900   2616.67   5252.88    0.635390   \n",
      "log_lik_pace[349]     -2.753730     -2.585070   2618.61   5392.97    0.635861   \n",
      "eta_pace[88,1]         2.950270      2.991790   2620.68   5305.38    0.636363   \n",
      "\n",
      "                      R_hat  \n",
      "sd_pace_away       1.000380  \n",
      "sd_pace_home       1.001800  \n",
      "sd_def_3m          1.002650  \n",
      "sd_def_ftm         1.001730  \n",
      "sd_q_2m            1.000580  \n",
      "sd_q_2a            1.000910  \n",
      "lp__               1.002950  \n",
      "pace_away[45]      1.000420  \n",
      "sd_q_3a            1.002220  \n",
      "log_lik_pace[350]  1.000250  \n",
      "eta_pace[88,2]     1.000250  \n",
      "mu_poss[88,2]      1.000250  \n",
      "log_lik_pace[351]  0.999958  \n",
      "log_lik_pace[352]  1.000110  \n",
      "mu_poss[88,3]      0.999956  \n",
      "eta_pace[88,3]     0.999956  \n",
      "eta_pace[88,4]     1.000110  \n",
      "mu_poss[88,4]      1.000110  \n",
      "log_lik_pace[349]  1.000140  \n",
      "eta_pace[88,1]     1.000150  \n"
     ]
    }
   ],
   "source": [
    "# Depois do treino (com o objeto 'fit')\n",
    "print(fit.diagnose())  # checa E-BFMI, treedepth hits, divergências por cadeia\n",
    "\n",
    "summ = fit.summary()\n",
    "# Itens com Rhat > 1.01\n",
    "print(summ.query(\"R_hat > 1.01\").sort_values(\"R_hat\").tail(20))\n",
    "# Itens com ESS baixo (sinais de mistura ruim)\n",
    "print(summ.nsmallest(20, \"ESS_bulk\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d75ba21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
